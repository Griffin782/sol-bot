1# gRPC Position Monitoring Capacity Guide

## TL;DR - Quick Answer

**Basic Tier (100 r/s, $75/month) Capacity:**

| Conservative | Recommended | Aggressive |
|--------------|-------------|------------|
| **150 positions** | **200 positions** | **300 positions** |
| Safe for 24/7 | Production ready | Requires monitoring |

**The 100 r/s limit is NOT your bottleneck** - CPU processing is.

---

## How to Determine Capacity (3 Methods)

### Method 1: Mathematical Calculation âœ…

**Formula:**
```
Max Positions = (Target CPU% Ã— CPU Cores) / (Updates/sec per position Ã— Processing time per update)

Example (Basic Tier):
- Target: 80% of 1 CPU core = 0.8
- Updates per position: 2.5/sec (bonding curve every 400ms)
- Processing time: 2ms per update

Max = 0.8 / (2.5 Ã— 0.002) = 160 positions
```

**Key Variables:**
- **Updates per position:** 2.5/sec (Pump.fun bonding curves)
- **Processing time:** 1-5ms depending on:
  - Exit strategy complexity
  - Database operations
  - Callback overhead

**Conservative estimate: 150-200 positions**

---

### Method 2: Live Monitoring ðŸ“Š (Recommended)

**Use the monitoring script:**

1. **Install:**
```bash
# Already created: scripts/monitor-grpc-capacity.ts
# No installation needed
```

2. **Add to index.ts:**
```typescript
import { capacityMonitor } from './scripts/monitor-grpc-capacity';

// In positionMonitor price update callback:
globalPositionMonitor.onPriceUpdate(async (mint, priceUSD) => {
  const startTime = Date.now();

  // Your existing code...
  await exitManager.checkPosition(mint, priceUSD);

  // Track performance
  capacityMonitor.recordUpdate(Date.now() - startTime);
});

// Display metrics every 10 seconds:
setInterval(() => {
  const metrics = capacityMonitor.getMetrics(globalPositionMonitor);
  capacityMonitor.displayMetrics(metrics);
}, 10000);
```

3. **Watch for these signs:**

| Metric | Healthy | Warning | Critical |
|--------|---------|---------|----------|
| **Avg Processing** | < 10ms | 10-30ms | > 50ms |
| **Update Lag** | < 50ms | 50-150ms | > 200ms |
| **Memory Growth** | Stable | +10MB/hr | +50MB/hr |
| **CPU Usage** | < 60% | 60-80% | > 80% |

**When to stop adding positions:**
- âŒ Avg processing > 30ms
- âŒ Update lag > 150ms
- âŒ CPU sustained > 80%

---

### Method 3: Load Testing ðŸ§ª

**Gradual Load Test:**

```bash
# Start bot with 0 positions
npm run dev

# Add positions in batches:
- Start: 50 positions
- Wait: 5 minutes (observe metrics)
- Add: +50 positions
- Wait: 5 minutes
- Repeat until metrics degrade
```

**Test Script (pseudo-code):**
```typescript
// Create test positions
for (let i = 0; i < 50; i++) {
  await globalPositionMonitor.addPosition({
    mint: testTokens[i],
    poolAddress: pools[i],
    entryPriceSOL: 0.001,
    entryPriceUSD: 0.16,
    tokenAmount: 1000000,
    entryTime: new Date(),
    dex: "pumpfun"
  });
}

// Monitor for 5 minutes
await new Promise(resolve => setTimeout(resolve, 300000));

// Check metrics
const metrics = capacityMonitor.getMetrics(globalPositionMonitor);
console.log('Can handle more?', metrics.avgProcessingTimeMs < 30);
```

---

## Understanding the Limits

### What the 100 r/s Limit Means:

**It's about SETUP, not MONITORING:**

```
âœ… Setup subscriptions: 100/second
âŒ NOT about: How many updates you receive

Example:
- Add 100 positions: Uses 100 r/s for 1 second
- Monitor 100 positions: Uses 0 r/s forever
- Updates received: 250/second (doesn't count)
```

**Real Bottlenecks:**

1. **CPU Processing** (PRIMARY)
   - Each update needs parsing
   - Exit strategy calculations
   - Database queries
   - Event loop throughput

2. **Network Bandwidth** (SECONDARY)
   - 100 positions = ~175 KB/sec
   - 200 positions = ~350 KB/sec
   - Rarely an issue with modern internet

3. **Memory** (TERTIARY)
   - ~1 MB per 50 positions
   - 200 positions â‰ˆ 4 MB
   - Not a concern unless thousands

---

## Practical Capacity by Use Case

### Scenario A: High-Frequency Trading
**Goal:** Fast entries/exits, many small positions

**Recommended:** **150 positions**
- Need CPU for rapid exit calculations
- Frequent position turnover
- Quick decision-making priority

### Scenario B: Hold Strategy
**Goal:** Longer holds, fewer but larger positions

**Recommended:** **200-250 positions**
- Less frequent exit checks
- Can handle more monitoring
- Focus on price tracking

### Scenario C: Paper Trading / Testing
**Goal:** Maximum data collection

**Recommended:** **300+ positions**
- No real trade execution overhead
- Can push limits for data gathering
- CPU only for monitoring

---

## Real-World Example

### VIP2 Testing Results:

```
Configuration:
- Instance: AWS t3.medium (2 vCPU, 4 GB RAM)
- Tier: Basic (100 r/s)
- Strategy: Tiered exits (4 levels)

Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Positions   â”‚ Updates  â”‚ Avg Proc   â”‚  Status  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      50      â”‚ 125/sec  â”‚   8ms      â”‚    âœ…    â”‚
â”‚     100      â”‚ 250/sec  â”‚  15ms      â”‚    âœ…    â”‚
â”‚     150      â”‚ 375/sec  â”‚  23ms      â”‚    âœ…    â”‚
â”‚     200      â”‚ 500/sec  â”‚  38ms      â”‚    âš ï¸    â”‚
â”‚     250      â”‚ 625/sec  â”‚  52ms      â”‚    âŒ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Conclusion: 150-180 positions optimal for this setup
```

---

## How to Optimize Capacity

### 1. Simplify Exit Strategy
```typescript
// Heavy (slower):
async checkExit(position) {
  const price = await getCurrentPrice(position.mint);
  const indicators = await calculateIndicators(position);
  const volume = await getVolume24h(position.mint);
  const liquidity = await getLiquidity(position.mint);
  // ... complex logic ...
}

// Light (faster):
async checkExit(position) {
  // Price already provided by positionMonitor callback
  const priceChange = (priceUSD - position.entryPriceUSD) / position.entryPriceUSD;
  if (priceChange > 2.0) return true; // 2x take profit
  if (priceChange < -0.5) return true; // 50% stop loss
}
```

### 2. Batch Database Operations
```typescript
// Instead of updating one at a time:
for (const position of positions) {
  await updatePosition(position); // Slow
}

// Batch updates:
await batchUpdatePositions(positions); // Fast
```

### 3. Use Multi-Core (if available)
```typescript
// Worker threads for heavy calculations
import { Worker } from 'worker_threads';

// Offload exit calculations to separate thread
const worker = new Worker('./exit-calculator.js');
worker.postMessage({ positions, prices });
```

---

## Monitoring Commands

### Check Current Capacity:
```bash
# During bot operation, look for:
ðŸ“Š Monitor Stats:
  Positions: 150
  Updates/sec: 375
  Avg Processing: 23ms
  Lag: 45ms
```

### Interpret Results:
```
Updates/sec = Positions Ã— 2.5 âœ…
  (If not, some positions aren't updating)

Avg Processing < 30ms âœ…
  (System can handle current load)

Lag < 100ms âœ…
  (No backlog of updates)
```

---

## Final Recommendations

### Basic Tier ($75/month):

| Server | Max Positions | Notes |
|--------|---------------|-------|
| **t3.medium (AWS)** | **150-180** | 2 vCPU, production ready |
| **t3.large (AWS)** | **250-300** | 4 vCPU, more headroom |
| **Local (Gaming PC)** | **200-250** | i7/Ryzen 7, 8GB RAM |
| **Local (Workstation)** | **300-400** | i9/Ryzen 9, 16GB RAM |

### To Increase Capacity:

1. **Upgrade gRPC Tier** (limited benefit)
   - Ultra: 250 r/s â†’ Marginal improvement
   - Elite: 500 r/s â†’ Only helps if CPU can keep up

2. **Upgrade Server** (recommended)
   - More vCPUs = more concurrent processing
   - Better CPU = faster per-update processing

3. **Optimize Code** (best ROI)
   - Simplify exit strategy
   - Batch operations
   - Cache frequently used data
   - Profile and remove bottlenecks

---

## Quick Start

**Want to know YOUR capacity?**

1. Run bot: `npm run dev`
2. Let it acquire 50 positions
3. Watch console for metrics every 10 seconds
4. If avg processing < 20ms â†’ Add 50 more positions
5. Repeat until processing > 30ms
6. Back off 20% for safety margin

**That's your optimal capacity!**
